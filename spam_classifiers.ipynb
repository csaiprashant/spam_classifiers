{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv('spam.csv')\n",
    "\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of spam instances\n",
    "\n",
    "len(spam_data[spam_data['target'] == 1]) / len(spam_data) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest Token in the Vocabulary is com1win150ppmx3age16subscription\n"
     ]
    }
   ],
   "source": [
    "# Longest Token in the vocabulary\n",
    "\n",
    "vec = CountVectorizer().fit(X_train)\n",
    "\n",
    "features = np.array(vec.get_feature_names())\n",
    "token_lengths = list(map(len, features))\n",
    "print('Longest Token in the Vocabulary is {}'.format(features[np.argmax(token_lengths)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "\n",
    "Fit and transform the training data `X_train` using a Count Vectorizer with default parameters.\n",
    "\n",
    "Next, fit a fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1`. Report the area under the curve (AUC) score using the transformed test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score of Multinomial Naive Bayes model is 0.9720812182741116\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "vec = CountVectorizer().fit(X_train)\n",
    "\n",
    "X_train_vec = vec.transform(X_train)\n",
    "model = MultinomialNB(alpha=0.1)\n",
    "model.fit(X_train_vec, y_train)\n",
    "predictions = model.predict(vec.transform(X_test))\n",
    "print('AUC score of Multinomial Naive Bayes model is {}'.format(roc_auc_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with smallest TF-IDF\n",
      "aaniye          0.074475\n",
      "athletic        0.074475\n",
      "chef            0.074475\n",
      "companion       0.074475\n",
      "courageous      0.074475\n",
      "dependable      0.074475\n",
      "determined      0.074475\n",
      "exterminator    0.074475\n",
      "healer          0.074475\n",
      "listener        0.074475\n",
      "organizer       0.074475\n",
      "pest            0.074475\n",
      "psychiatrist    0.074475\n",
      "psychologist    0.074475\n",
      "pudunga         0.074475\n",
      "stylist         0.074475\n",
      "sympathetic     0.074475\n",
      "venaam          0.074475\n",
      "diwali          0.091250\n",
      "mornings        0.091250\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Features with largest TF-IDF\n",
      "146tf150p    1.000000\n",
      "645          1.000000\n",
      "anything     1.000000\n",
      "anytime      1.000000\n",
      "beerage      1.000000\n",
      "done         1.000000\n",
      "er           1.000000\n",
      "havent       1.000000\n",
      "home         1.000000\n",
      "lei          1.000000\n",
      "nite         1.000000\n",
      "ok           1.000000\n",
      "okie         1.000000\n",
      "thank        1.000000\n",
      "thanx        1.000000\n",
      "too          1.000000\n",
      "where        1.000000\n",
      "yup          1.000000\n",
      "tick         0.980166\n",
      "blank        0.932702\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Exploratory data analysis using TF-IDF Vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer().fit(X_train)\n",
    "    \n",
    "X_train_vec = vec.transform(X_train)\n",
    "\n",
    "features = np.array(vec.get_feature_names())\n",
    "\n",
    "sorted_tfidf = X_train_vec.max(0).toarray()[0].argsort()\n",
    "\n",
    "small_index = features[sorted_tfidf[:20]]\n",
    "small_value = np.sort(X_train_vec.max(0).toarray()[0])[:20]\n",
    "small_final_index = np.concatenate((np.sort(small_index[small_value == min(small_value)]), small_index[small_value != min(small_value)]))\n",
    "\n",
    "large_index = features[sorted_tfidf[:-21:-1]]\n",
    "large_value = np.sort(X_train_vec.max(0).toarray()[0])[:-21:-1]\n",
    "large_final_index = np.concatenate((np.sort(large_index[large_value == max(large_value)]), large_index[large_value != max(large_value)]))\n",
    "\n",
    "small = pd.Series(small_value,index=small_final_index)\n",
    "large = pd.Series(large_value,index=large_final_index)\n",
    "\n",
    "print('Features with smallest TF-IDF')\n",
    "print(small)\n",
    "print('\\n')\n",
    "print('Features with largest TF-IDF')\n",
    "print(large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **3**.\n",
    "\n",
    "Then fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1` and compute the area under the curve (AUC) score using the transformed test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score of Multinomial Naive Bayes model is 0.9416243654822335\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "X_train_vec = vec.transform(X_train)\n",
    "model = MultinomialNB(alpha=0.1)\n",
    "model.fit(X_train_vec, y_train)\n",
    "predictions = model.predict(vec.transform(X_test))\n",
    "\n",
    "print('AUC score of Multinomial Naive Bayes model is {}'.format(roc_auc_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of spam text = 138.8661311914324\n",
      "Average length of non-spam text = 71.02362694300518\n"
     ]
    }
   ],
   "source": [
    "# Use length of text as an additional feature\n",
    "\n",
    "length_spam = list(map(len,spam_data['text'][spam_data.target == 1]))\n",
    "length_not_spam = list(map(len,spam_data['text'][spam_data.target == 0]))\n",
    "\n",
    "print('Average length of spam text = {}'.format(np.mean(length_spam)))\n",
    "print('Average length of non-spam text = {}'.format(np.mean(length_not_spam)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add additional features\n",
    "\n",
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "\n",
    "Fit and transform the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **5**.\n",
    "\n",
    "Using this document-term matrix and an additional feature, **the length of document (number of characters)**, fit a Support Vector Classification model with regularization `C=10000`. Then compute the area under the curve (AUC) score using the transformed test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score of SVM model is 0.9581366823421557\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "vec = TfidfVectorizer(min_df=5)\n",
    "\n",
    "X_train_vec = vec.fit_transform(X_train)\n",
    "X_train_trans = add_feature(X_train_vec, X_train.str.len())\n",
    "\n",
    "X_test_vec = vec.transform(X_test)\n",
    "X_test_trans = add_feature(X_test_vec, X_test.str.len())\n",
    "\n",
    "clf = SVC(C=10000, gamma='auto')\n",
    "clf.fit(X_train_trans, y_train)\n",
    "y_predicted = clf.predict(X_test_trans)\n",
    "\n",
    "print('AUC score of SVM model is {}'.format(roc_auc_score(y_test, y_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # of digits in spam text = 15.759036144578314\n",
      "Average # of digits in non-spam text = 0.2992746113989637\n"
     ]
    }
   ],
   "source": [
    "# Use average number of digits per text as additional feature\n",
    "\n",
    "import re\n",
    "\n",
    "spam = [re.findall(\"[0-9]\", i) for i in spam_data['text'][spam_data.target == 1]]\n",
    "non_spam = [re.findall(\"[0-9]\", i) for i in spam_data['text'][spam_data.target == 0]]\n",
    "\n",
    "print('Average # of digits in spam text = {}'.format(np.mean(list(map(len, spam)))))\n",
    "print('Average # of digits in non-spam text = {}'.format(np.mean(list(map(len, non_spam)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **word n-grams from n=1 to n=3** (unigrams, bigrams, and trigrams).\n",
    "\n",
    "Using this document-term matrix and the following additional features:\n",
    "* the length of document (number of characters)\n",
    "* **number of digits per document**\n",
    "\n",
    "fit a Logistic Regression model with regularization `C=100`. Then compute the area under the curve (AUC) score using the transformed test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score of Logistic Regression model is 0.9678709064054463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vec = TfidfVectorizer(min_df=5, ngram_range=[1, 3])\n",
    "    \n",
    "X_train_vec = vec.fit_transform(X_train)\n",
    "X_train_trans = add_feature(X_train_vec, [X_train.str.len(), X_train.apply(lambda x: len(''.join([a for a in x if a.isdigit()])))])\n",
    "\n",
    "X_test_vec = vec.transform(X_test)\n",
    "X_test_trans = add_feature(X_test_vec, [X_test.str.len(), X_test.apply(lambda x: len(''.join([a for a in x if a.isdigit()])))])\n",
    "\n",
    "clf = LogisticRegression(C=100, solver='liblinear').fit(X_train_trans, y_train)\n",
    "y_predicted = clf.predict(X_test_trans)\n",
    "\n",
    "print('AUC score of Logistic Regression model is {}'.format(roc_auc_score(y_test, y_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # of non-word characters in spam text = 29.041499330655956\n",
      "Average # of non-word characters in non-spam text = 17.29181347150259\n"
     ]
    }
   ],
   "source": [
    "# Use number of non-word characters as additional features\n",
    "\n",
    "spam_data['alpha_num'] = spam_data['text'].str.findall(r'(\\W)').str.len()\n",
    "\n",
    "spam_non_word = np.mean(spam_data['alpha_num'][spam_data.target == 1])\n",
    "non_spam_non_word = np.mean(spam_data['alpha_num'][spam_data.target == 0])\n",
    "\n",
    "print('Average # of non-word characters in spam text = {}'.format(spam_non_word))\n",
    "print('Average # of non-word characters in non-spam text = {}'.format(non_spam_non_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5\n",
    "\n",
    "Fit and transform the training data X_train using a Count Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **character n-grams from n=2 to n=5.**\n",
    "\n",
    "To tell Count Vectorizer to use character n-grams Passing in `analyzer='char_wb'` to the Count Vectorizer tells it to use character n-grams by forcing it to create character n-grams only from text inside word boundaries. This should make the model more robust to spelling mistakes.\n",
    "\n",
    "Using this document-term matrix and the following additional features:\n",
    "* the length of document (number of characters)\n",
    "* number of digits per document\n",
    "* **number of non-word characters (anything other than a letter, digit or underscore.)**\n",
    "\n",
    "Fit a Logistic Regression model with regularization C=100. Then compute the area under the curve (AUC) score using the transformed test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score of Logistic Regression model is 0.9788593110707434\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(min_df=5, ngram_range=[2, 5], analyzer='char_wb')\n",
    "    \n",
    "X_train_vec = vec.fit_transform(X_train)\n",
    "X_train_trans = add_feature(X_train_vec, [X_train.str.len(), \n",
    "                                          X_train.apply(lambda x: len(''.join([a for a in x if a.isdigit()]))),\n",
    "                                          X_train.str.findall(r'(\\W)').str.len()])\n",
    "X_test_vec = vec.transform(X_test)\n",
    "X_test_trans = add_feature(X_test_vec, [X_test.str.len(),\n",
    "                                        X_test.apply(lambda x: len(''.join([a for a in x if a.isdigit()]))),\n",
    "                                        X_test.str.findall(r'(\\W)').str.len()])\n",
    "\n",
    "clf = LogisticRegression(C=100, solver='liblinear').fit(X_train_trans, y_train)\n",
    "y_predicted = clf.predict(X_test_trans) \n",
    "auc_score = roc_auc_score(y_test, y_predicted)\n",
    "\n",
    "print('AUC score of Logistic Regression model is {}'.format(roc_auc_score(y_test, y_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with smallest coefficients:\n",
      "['. ', '..', '? ', ' i', ' y', ' go', ':)', ' h', 'go', ' m']\n",
      "\n",
      "\n",
      "Features with largest coefficients:\n",
      "['digit_count', 'ne', 'ia', 'co', 'xt', ' ch', 'mob', ' x', 'ww', 'ar']\n"
     ]
    }
   ],
   "source": [
    "features = np.array(vec.get_feature_names() + ['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "\n",
    "sorted_coef_index = clf.coef_[0].argsort()\n",
    "smallest = list(features[sorted_coef_index[:10]])\n",
    "largest = list(features[sorted_coef_index[:-11:-1]])\n",
    "\n",
    "print('Features with smallest coefficients:')\n",
    "print(smallest)\n",
    "print('\\n')\n",
    "print('Features with largest coefficients:')\n",
    "print(largest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
